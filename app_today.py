import os
import sys
from pathlib import Path
import configparser
from datetime import datetime, timezone, timedelta
from zoneinfo import ZoneInfo

import pandas as pd
import streamlit as st
from sqlalchemy import create_engine

from urllib.parse import quote_plus

import json
from sqlalchemy import text
from decimal import Decimal
import numpy as np
from datetime import date

# å‰åºæŸ¥è©¢é‚è¼¯æ¨¡çµ„
from prev_process import fetch_prev_process_and_upstream
from material_prep import get_material_production_progress_list_of_kanban

# from call_llm_api import analyze_overdue_with_llm
from call_llm_api import analyze_overdue_with_llm

# RAGï¼ˆé•·æœŸæ”¹å–„ï¼‰æ¨¡çµ„ï¼šè‹¥å°šæœªå»ºç«‹æª”æ¡ˆï¼Œæä¾›å®‰å…¨é™ç´šå‡½å¼
try:
    from call_rag_api import get_long_term_improvements
except Exception:
    def get_long_term_improvements(*args, **kwargs):
        return {"long_term_actions": [], "raw": "", "error": "call_rag_api module not found"}
# --- JSON sanitization helper ---
def _json_safe(obj):
    """Recursively convert objects to JSON-serializable types (Decimal, numpy, pandas Timestamps, sets, etc.)."""
    try:
        # pandas/numpy aware NaN
        import pandas as pd  # local reference
    except Exception:
        pd = None

    if isinstance(obj, dict):
        return {k: _json_safe(v) for k, v in obj.items()}
    if isinstance(obj, (list, tuple)):
        return type(obj)(_json_safe(v) for v in obj)
    if isinstance(obj, set):
        return [_json_safe(v) for v in obj]
    if isinstance(obj, Decimal):
        try:
            return float(obj)
        except Exception:
            return str(obj)
    if pd is not None and isinstance(obj, pd.Timestamp):
        try:
            return obj.isoformat()
        except Exception:
            return str(obj)
    if isinstance(obj, (datetime, date)):
        try:
            return obj.isoformat()
        except Exception:
            return str(obj)
    if isinstance(obj, (np.integer,)):
        return int(obj)
    if isinstance(obj, (np.floating,)):
        return float(obj)
    # map pandas NA/NaT to None
    try:
        import math
        if obj is None:
            return None
        # handle pandas NA without importing pandas types explicitly
        if hasattr(obj, "_isnan") and obj._isnan():
            return None
        if isinstance(obj, float) and math.isnan(obj):
            return None
    except Exception:
        pass
    return obj

# ----------------------------
# 1) è®€å– config.ini
# ----------------------------
def load_config():
    """
    è®€å– config.iniï¼Œå„ªå…ˆé †åºï¼š
    1) ç’°å¢ƒè®Šæ•¸ CONFIG_PATH æŒ‡å®šçš„è·¯å¾‘
    2) èˆ‡æ­¤æª”åŒå±¤çš„ ./config.ini
    3) å°ˆæ¡ˆæ ¹ç›®éŒ„æˆ–ç¿’æ…£çš„ fallbackï¼ˆå¯è‡ªè¡ŒåŠ ï¼‰
    """
    cfg_path = os.getenv("CONFIG_PATH")
    if cfg_path:
        cfg = Path(cfg_path)
    else:
        # é è¨­æ‰¾è·Ÿæ­¤æª”åŒä¸€å±¤çš„ config.ini
        cfg = Path(__file__).resolve().parent / "config.ini"

    if not cfg.exists():
        st.error(f"æ‰¾ä¸åˆ°è¨­å®šæª”ï¼š{cfg}")
        st.stop()

    cp = configparser.ConfigParser()
    return cp, cfg

cp, cfg = load_config()

def build_db_url(db_conf: configparser.SectionProxy) -> str:
    """
    ç”± config.ini ç”Ÿæˆ SQLAlchemy é€£ç·šå­—ä¸²
    åªé‡å° MySQL/MariaDB + PyMySQLï¼ˆdriver=mysql+pymysqlï¼‰
    """
    driver   = db_conf.get("driver", "mysql+pymysql")
    host     = db_conf.get("host", "127.0.0.1").strip()
    port_raw = db_conf.get("port", "3306")
    user     = quote_plus(db_conf.get("user", ""))       # ä½¿ç”¨è€…åç¨±åš URL ç·¨ç¢¼
    password = quote_plus(db_conf.get("password", ""))   # å¯†ç¢¼åš URL ç·¨ç¢¼ï¼ˆè™•ç†ç‰¹æ®Šå­—å…ƒï¼‰
    database = db_conf.get("database", "")
    charset  = db_conf.get("charset", "utf8mb4")
    query    = db_conf.get("query", "")  # å¯å¡« ssl=..., ssl_disabled=... ç­‰

    # host ä¸å¯åŒ…å« SSH ç”¨æ³•çš„ '@'ï¼ˆé˜²æ­¢æŠŠ 'user@host' èª¤æ”¾åœ¨ host æ¬„ä½ï¼‰
    if "@" in host:
        st.error("config.ini çš„ host ä¸å¯åŒ…å« '@'ã€‚è«‹æŠŠ SSH å¸³è™Ÿæ”¾åœ¨ SSH æŒ‡ä»¤ï¼Œä¸è¦æ”¾åœ¨ hostï¼ˆä¾‹ï¼šssh -L... your_ssh_user@hostï¼‰ã€‚")
        st.stop()

    # è½‰å‹ portï¼ˆè‹¥å¡«äº†éæ•¸å­—ï¼Œäº¤çµ¦ SQLAlchemy ä¹Ÿèƒ½è™•ç†ï¼›é€™è£¡ç›¡é‡è½‰ intï¼‰
    try:
        port = int(port_raw)
    except Exception:
        port = port_raw  # ä¿ç•™åŸå­—ä¸²

    base = f"{driver}://{user}:{password}@{host}:{port}/{database}?charset={charset}"
    if query:
        # ç§»é™¤å‰å° ? æˆ– &ï¼Œé¿å…é‡è¦†
        query = query.lstrip("?&")
        base = f"{base}&{query}"
    return base

def get_db_config(cp, cfg):
    """
    """
    cp.read(cfg, encoding="utf-8")
    if "database" not in cp:
        st.error("config.ini ç¼ºå°‘ [database] å€æ®µ")
        st.stop()
    return cp["database"]

def get_db_leanplay_supp_config(cp, cfg):
    """
    """
    cp.read(cfg, encoding="utf-8")
    if "database_leanplay_supp" not in cp:
        st.error("config.ini ç¼ºå°‘ [database_leanplay_supp] å€æ®µ")
        st.stop()
    return cp["database_leanplay_supp"]
# ----------------------------
# 2) Streamlit é é¢è¨­å®š
# ----------------------------
st.set_page_config(page_title="ä»Šæ—¥ç”Ÿç”¢çœ‹æ¿ç›£æ§", layout="wide")
st.title("ä»Šæ—¥ç”Ÿç”¢çœ‹æ¿ç›£æ§")

db_conf = get_db_config(cp, cfg)
DB_URL = build_db_url(db_conf)
timezone_name = db_conf.get("timezone", "DBé è¨­")

# ä½¿ç”¨è€…å¯æ–¼ config.ini è¨­å®š timezoneï¼Œå¦‚æœªè¨­å®šå‰‡é è¨­ä½¿ç”¨ Asia/Taipei ä½œç‚ºé¡¯ç¤ºæ™‚å€
display_tz = timezone_name if (timezone_name and timezone_name not in ("DBé è¨­", "")) else "Asia/Taipei"

# å»ºç«‹ Engineï¼ˆä½¿ç”¨é€£ç·šå­˜æ´»æª¢æŸ¥ï¼‰
engine = create_engine(DB_URL, pool_pre_ping=True)

# é¡¯ç¤ºæ™‚é–“ï¼ˆUI é¡¯ç¤ºç”¨é€”ï¼‰
timezone_name = db_conf.get("timezone", "DBé è¨­")

try:
    if display_tz:
        now_local = datetime.now(ZoneInfo(display_tz))
    else:
        now_local = datetime.now()
except Exception:
    # å›ºå®šåç§»ï¼ˆç„¡ DST è¦å‰‡ï¼‰
    if display_tz == "Asia/Taipei":
        now_local = datetime.now(timezone(timedelta(hours=8)))
    else:
        now_local = datetime.now()

# ----------------------------
# 4) è³‡æ–™è®€å–ï¼ˆä½¿ç”¨ Viewï¼‰
# ----------------------------
@st.cache_data(ttl=60)
def load_today_views():
    with engine.connect() as conn:
        summary = pd.read_sql("SELECT * FROM v_today_kanban_planned_vs_actual", conn)
        detail  = pd.read_sql("SELECT * FROM v_today_kanban_start_detail", conn)
    return summary, detail


try:
    summary_df, detail_df = load_today_views()
except Exception as e:
    st.error(f"è®€å–è³‡æ–™å¤±æ•—ï¼š{e}")
    st.stop()

# ----------------------------
# å°‡æ˜ç´°çš„æ™‚é–“æ¬„ä½è½‰ç‚ºé¡¯ç¤ºæ™‚å€ï¼ˆå‡è¨­ DB å„²å­˜ç‚º UTCï¼‰
# ----------------------------
time_cols = [
    "expected_start_time",
    "actual_start_time",
    "actual_end_time",
]

for col in time_cols:
    if col in detail_df.columns:
        s = pd.to_datetime(detail_df[col], errors="coerce")
        try:
            # è‹¥ç‚º naive æ™‚é–“ï¼Œå…ˆç•¶ä½œ UTCï¼Œå†è½‰æˆ display_tz
            if s.dt.tz is None:
                s = s.dt.tz_localize("UTC").dt.tz_convert(display_tz)
            else:
                s = s.dt.tz_convert(display_tz)
            detail_df[col] = s.dt.strftime("%Y-%m-%d %H:%M:%S")
        except Exception:
            # è½‰æ›å¤±æ•—æ™‚ï¼Œä¿ç•™ç‚ºå­—ä¸²ä»¥é¿å…ä¸­æ–·
            detail_df[col] = s.astype(str)

# ----------------------------
# è³‡æ–™æ›´æ–°æ™‚é–“ï¼ˆä»¥ fact_production_job çš„ MAX(load_dts) ç‚ºæº–ï¼‰
# ----------------------------
asof_str = None
try:
    with engine.connect() as conn:
        asof_df = pd.read_sql("SELECT MAX(load_dts) AS asof FROM fact_production_job", conn)
        if not asof_df.empty and pd.notna(asof_df.loc[0, "asof"]):
            asof_val = pd.to_datetime(asof_df.loc[0, "asof"])
            # load_dts ç‚º UTCï¼Œå†è½‰ display_tzï¼ˆé¿å… DB/é€£ç·šæ™‚å€ä¸ä¸€è‡´ï¼‰
            try:
                if getattr(asof_val, "tzinfo", None) is None:
                    asof_val = asof_val.tz_localize("UTC").tz_convert(display_tz)
                else:
                    asof_val = asof_val.tz_convert(display_tz)
            except Exception:
                # é€€å›ï¼šç›´æ¥å­—ä¸²åŒ–ï¼ˆä»é™„ä¸Šçµ±ä¸€çš„æ™‚å€æ¨™ç±¤ï¼‰
                asof_str = f"{asof_val.strftime('%Y-%m-%d %H:%M:%S') if hasattr(asof_val, 'strftime') else str(asof_val)} ({display_tz})"
            else:
                # çµ±ä¸€ç”¨æ–‡å­—æ¨™ç¤º "(display_tz)"ï¼Œé¿å…é¡¯ç¤ºç‚º CST
                asof_str = f"{asof_val.strftime('%Y-%m-%d %H:%M:%S')} ({display_tz})"
except Exception:
    asof_str = None

if asof_str:
    st.caption(f"è³‡æ–™æ›´æ–°æ™‚é–“ï¼š{asof_str}ï¼›é é¢æŸ¥è©¢ {now_local:%Y-%m-%d %H:%M:%S} ({display_tz})")
else:
    st.caption(f"è³‡æ–™æ›´æ–°æ™‚é–“ï¼šé é¢æŸ¥è©¢ {now_local:%Y-%m-%d %H:%M:%S} ({display_tz})")

# ----------------------------
# 3) ç¯©é¸å™¨ï¼ˆå‰ç«¯éæ¿¾ï¼‰
# ----------------------------

# ----------------------------
# 3.1) æ ¹æ“šæ˜ç´°å»ºç«‹é¸é …ä¸¦æ¸²æŸ“ä¸€æ¬¡ï¼ˆåŠ å…¥å·¥å–®ç¯©é¸ï¼‰
# ----------------------------
wc_opts   = sorted(detail_df["work_center_id"].dropna().unique().tolist()) if "work_center_id" in detail_df else []
proc_opts = sorted(detail_df["process_id"].dropna().unique().tolist()) if "process_id" in detail_df else []
part_opts = sorted(detail_df["part_no"].dropna().unique().tolist()) if "part_no" in detail_df else []
wo_opts   = sorted(detail_df["work_order_id"].dropna().unique().tolist()) if "work_order_id" in detail_df else []

frow = st.columns([1,1,1,1,1])
with frow[0]:
    filter_wc = st.multiselect("å·¥ä½œç«™ Work Center", options=wc_opts, default=[])
with frow[1]:
    filter_proc = st.multiselect("è£½ç¨‹ Process", options=proc_opts, default=[])
with frow[2]:
    filter_part = st.multiselect("æ–™è™Ÿ Part", options=part_opts, default=[])
with frow[3]:
    filter_wo = st.multiselect("å·¥å–® Work Order", options=wo_opts, default=[])
with frow[4]:
    only_overdue = st.toggle("åªçœ‹é€¾æœŸæœªé–‹å·¥", value=False)

# ----------------------------
# 5) KPI å¡ç‰‡
# ----------------------------
if not summary_df.empty:
    s = summary_df.iloc[0]
    kpi1, kpi2, kpi3, kpi4, kpi5, kpi6 = st.columns(6)
    with kpi1:
        st.metric(
            label="ä»Šæ—¥æ‡‰é–‹å·¥æ•¸ (çœ‹æ¿)",
            value=int(s.get("planned_kanbans_today", 0)),
            help="ä»Šæ—¥è¨ˆç•«è¦é–‹å·¥çš„çœ‹æ¿æ•¸é‡ï¼ˆä»¥ expected_start_time è½åœ¨ä»Šæ—¥ 00:00â€“24:00 ç‚ºæº–ï¼‰ã€‚"
        )
    with kpi2:
        st.metric(
            label="ä»Šæ—¥å¯¦éš›å·²é–‹å·¥ (çœ‹æ¿)",
            value=int(s.get("actual_started_kanbans_today", 0)),
            help="ä»Šæ—¥å¯¦éš›ç™¼ç”Ÿé–‹å·¥çš„çœ‹æ¿æ•¸é‡ï¼ˆä»¥ actual_start_time è½åœ¨ä»Šæ—¥ 00:00â€“24:00 ç‚ºæº–ï¼Œä¸è«–åŸè¨ˆç•«æ—¥ï¼‰ã€‚"
        )
    with kpi3:
        st.metric(
            label="æº–æ™‚é–‹å·¥",
            value=int(s.get("on_time_starts_today", 0)),
            help="åŒæ™‚æ»¿è¶³ï¼šè¨ˆç•«ä»Šæ—¥é–‹å·¥ï¼ˆexpected_start_time åœ¨ä»Šæ—¥ï¼‰ä¸”ä»Šæ—¥å¯¦éš›é–‹å·¥ï¼Œä¸¦ä¸” actual_start_time â‰¤ expected_start_timeã€‚"
        )
    with kpi4:
        st.metric(
            label="å»¶é²é–‹å·¥",
            value=int(s.get("late_starts_today", 0)),
            help="åŒæ™‚æ»¿è¶³ï¼šè¨ˆç•«ä»Šæ—¥é–‹å·¥ï¼ˆexpected_start_time åœ¨ä»Šæ—¥ï¼‰ä¸”ä»Šæ—¥å¯¦éš›é–‹å·¥ï¼Œä½† actual_start_time ï¼ expected_start_timeã€‚"
        )
    with kpi5:
        st.metric(
            label="é€¾æœŸæœªé–‹å·¥",
            value=int(s.get("overdue_not_started_so_far", 0)),
            help="æˆªè‡³ç›®å‰æ™‚é–“ï¼Œè¨ˆç•«æ™‚é–“å·²åˆ°ï¼ˆexpected_start_time â‰¤ ç¾åœ¨ï¼‰ä½†å°šæœªå¯¦éš›é–‹å·¥ï¼ˆactual_start_time ç‚ºç©ºï¼‰çš„çœ‹æ¿æ•¸ã€‚"
        )
    with kpi6:
        st.metric(
            label="æå‰å®Œå·¥ï¼ˆè¨ˆç•«ä»Šæ—¥ï¼‰",
            value=int(s.get("planned_today_but_finished_before_today", 0)),
            help="è¨ˆç•«ä»Šæ—¥æ‰é–‹å·¥ï¼Œä½†å¯¦éš›å®Œå·¥æ™‚é–“æ—©æ–¼ä»Šæ—¥ï¼ˆactual_end_time ï¼œ ä»Šæ—¥ 00:00ï¼‰ï¼Œä»£è¡¨ä»»å‹™å·²åœ¨å…ˆå‰æ—¥å­å®Œæˆã€‚"
        )

    planned = int(s.get("planned_kanbans_today", 0))
    actual  = int(s.get("actual_started_kanbans_today", 0))
    rate = (actual/planned) if planned else 0
    st.progress(min(max(rate, 0), 1.0), text=f"ä»Šæ—¥é–‹å·¥é”æˆç‡ï¼š{rate:.0%}")
    st.caption("**é”æˆç‡è¨ˆç®—æ–¹å¼**ï¼šä»Šæ—¥é–‹å·¥é”æˆç‡ = ä»Šæ—¥å¯¦éš›å·²é–‹å·¥(çœ‹æ¿) Ã· ä»Šæ—¥æ‡‰é–‹å·¥æ•¸(çœ‹æ¿)ã€‚ç•¶åˆ†æ¯ç‚º 0 æ™‚ï¼Œé¡¯ç¤ºç‚º 0%ã€‚")

st.divider()

# ----------------------------
# 6) æ˜ç´°è¡¨ + é«˜äº® + ä¸‹è¼‰ï¼ˆåˆ†é  + å…¨é‡åŒ¯å‡ºï¼‰
# ----------------------------
df = detail_df.copy()

# å‰ç«¯ç¯©é¸
if filter_wc:
    df = df[df["work_center_id"].isin(filter_wc)]
if filter_proc:
    df = df[df["process_id"].isin(filter_proc)]
if filter_part:
    df = df[df["part_no"].isin(filter_part)]
if filter_wo:
    df = df[df["work_order_id"].isin(filter_wo)]
if only_overdue and "overdue_not_started_so_far_flag" in df:
    df = df[df["overdue_not_started_so_far_flag"] == 1]

# æ¬„ä½æ”¹ç‚ºç¹é«”ä¸­æ–‡ï¼ˆåƒ…å°å·²å­˜åœ¨æ¬„ä½é€²è¡Œé‡å‘½åï¼‰
zh_map = {
    "kanban_id": "çœ‹æ¿ID",
    "work_order_id": "å·¥å–®è™Ÿ",
    "part_no": "æ–™è™Ÿ",
    "work_center_id": "å·¥ä½œç«™",
    "process_id": "è£½ç¨‹",
    "process_seq": "è£½ç¨‹åºè™Ÿ",
    "expected_start_time": "è¨ˆç•«é–‹å·¥æ™‚é–“",
    "actual_start_time": "å¯¦éš›é–‹å·¥æ™‚é–“",
    "produce_status": "ç”Ÿç”¢ç‹€æ…‹",
    "planned_today_flag": "ä»Šæ—¥è¨ˆç•«",
    "started_today_flag": "ä»Šæ—¥å¯¦éš›é–‹å·¥",
    "overdue_not_started_so_far_flag": "é€¾æœŸæœªé–‹å·¥",
    "started_on_time_today_flag": "æº–æ™‚é–‹å·¥",
    "started_late_today_flag": "å»¶é²é–‹å·¥"
}
display_cols_order = [
    "kanban_id","work_order_id","part_no","work_center_id",
    "process_id","process_seq","expected_start_time","actual_start_time",
    "planned_today_flag","started_today_flag","started_on_time_today_flag",
    "started_late_today_flag","overdue_not_started_so_far_flag","produce_status"
]

# æ’åºæ¬„ä½å­˜åœ¨æ‰æ’åºï¼ˆç©©å®šä¸”ä¸€è‡´ï¼‰
sort_cols = [c for c in ["work_center_id", "process_id", "process_seq", "expected_start_time"] if c in df.columns]
df_sorted = df.sort_values(sort_cols, na_position="last") if sort_cols else df

st.subheader("ä»Šæ—¥çœ‹æ¿æ˜ç´°")
st.caption("é¡è‰²ï¼šç´…=é€¾æœŸæœªé–‹å·¥ï¼›ç¶ =æº–æ™‚ï¼›é»ƒ=å»¶é²")


# ---------------- åˆ†é ç‹€æ…‹ï¼ˆç§»åˆ°è¡¨æ ¼ä¸Šæ–¹è¨ˆç®—ï¼Œæ§åˆ¶åœ¨è¡¨æ ¼ä¸‹æ–¹ï¼‰----------------
# ä»¥ session_state è¨˜ä½é ç¢¼èˆ‡æ¯é ç­†æ•¸ï¼ˆé¿å…æ›é å›è·³ï¼‰
if "detail_page_size" not in st.session_state:
    st.session_state.detail_page_size = 50
if "detail_page" not in st.session_state:
    st.session_state.detail_page = 1

page_size = int(st.session_state.detail_page_size)

total_rows  = len(df_sorted)
total_pages = max((total_rows - 1) // page_size + 1, 1)

# ä¿éšªï¼šé ç¢¼è½åœ¨æœ‰æ•ˆç¯„åœ
page = int(st.session_state.detail_page)
if page < 1:
    page = 1
elif page > total_pages:
    page = total_pages

start = (page - 1) * page_size
end   = min(start + page_size, total_rows)

# å–æœ¬é è³‡æ–™
display_cols = [c for c in display_cols_order if c in df_sorted.columns] or list(df_sorted.columns)
df_page = df_sorted.iloc[start:end][display_cols]
df_display = df_page.rename(columns={k: v for k, v in zh_map.items() if k in display_cols})

# é«˜äº®è¦å‰‡ï¼ˆæ²¿ç”¨åŸé‚è¼¯ï¼‰
def highlight_row(row):
    """
    æ ¹æ“šæ——æ¨™æ¬„ä½ä¸Šè‰²ã€‚æ”¯æ´è‹±/ä¸­æ–‡æ¬„ä½åç¨±ï¼š
      - overdue_not_started_so_far_flag / é€¾æœŸæœªé–‹å·¥  -> ç´…
      - started_on_time_today_flag       / æº–æ™‚é–‹å·¥    -> ç¶ 
      - started_late_today_flag          / å»¶é²é–‹å·¥    -> é»ƒ
    """
    def get_flag(r, en, zh):
        val = r.get(en, r.get(zh, 0))
        try:
            return int(val) if val is not None else 0
        except Exception:
            return 1 if str(val).strip() in ("1", "True", "true", "Y") else 0
    
    overdue = get_flag(row, "overdue_not_started_so_far_flag", "é€¾æœŸæœªé–‹å·¥")
    ontime  = get_flag(row, "started_on_time_today_flag", "æº–æ™‚é–‹å·¥")
    late    = get_flag(row, "started_late_today_flag", "å»¶é²é–‹å·¥")

    if overdue == 1:
        return ["background-color: #ffe5e5"] * len(row)   # é€¾æœŸï¼šç´…
    if ontime == 1:
        return ["background-color: #eaffe5"] * len(row)   # æº–æ™‚ï¼šç¶ 
    if late == 1:
        return ["background-color: #fff6e0"] * len(row)   # å»¶é²ï¼šé»ƒ
    return [""] * len(row)

# è¡¨æ ¼å…§å–®åˆ—é¸å–è§¸ç™¼ï¼ˆsingle-rowï¼‰ã€‚é¸åˆ°å¾Œæœƒ rerunï¼Œä¸¦åœ¨ä¸‹æ–¹ç”¢ç”Ÿåˆ†æ payloadã€‚
selection = st.dataframe(
    df_display.style.apply(highlight_row, axis=1),
    width='stretch',
    height=520,
    on_select="rerun",
    selection_mode="single-row",
    key="detail_table"
)

# ä½¿ç”¨èªªæ˜
st.caption("ğŸ’¡ åœ¨è¡¨æ ¼å…§é»é¸ä»»ä¸€åˆ—å³å¯åˆ†æï¼›ç›®å‰åƒ…é‡å°ã€é€¾æœŸæœªé–‹å·¥ã€çš„åˆ—ç”¢ç”Ÿåˆ†æã€‚")

# å–å¾—é¸å–çµæœï¼šæ”¯æ´ st.dataframe ç›´æ¥å›å‚³æˆ–ç¶“ç”± session_state å–å¾—
sel_rows = []
try:
    # 1) ç›´æ¥å›å‚³ï¼ˆæŸäº›ç‰ˆæœ¬ç›´æ¥å› dictï¼‰
    if isinstance(selection, dict):
        if "rows" in selection and isinstance(selection["rows"], list):
            sel_rows = selection["rows"]
        elif "selection" in selection and isinstance(selection["selection"], dict):
            sel_rows = selection["selection"].get("rows", []) or []
    # 2) ç”± session_state å–å¾—ï¼ˆè¼ƒæ–°ç‰ˆæœ¬æœƒæŠŠ selection æ”¾åœ¨ key ä¸‹ï¼‰
    if not sel_rows:
        tbl_state = st.session_state.get("detail_table", {})
        if isinstance(tbl_state, dict):
            if "rows" in tbl_state and isinstance(tbl_state["rows"], list):
                sel_rows = tbl_state["rows"]
            elif "selection" in tbl_state and isinstance(tbl_state["selection"], dict):
                sel_rows = tbl_state["selection"].get("rows", []) or []
except Exception:
    sel_rows = []

# è‹¥æœ¬æ¬¡æ²’æœ‰é¸å–ä»»ä½•åˆ—ï¼Œæ¸…ç©ºæš«å­˜ payloadï¼ˆé¿å…èª¤ç”¨èˆŠè³‡æ–™ï¼‰
if not sel_rows:
    st.session_state.pop("current_payload", None)
    st.session_state.pop("current_row_key", None)

# ---------------- åˆ†é æ§åˆ¶ï¼ˆæ¬åˆ°è¡¨æ ¼ä¸‹æ–¹ï¼‰----------------
ctrl = st.container()
with ctrl:
    c = st.columns([1.6, 2.4, 3.0])
    with c[0]:
        new_size = st.selectbox("æ¯é ç­†æ•¸", options=[50, 100, 200], index=[50,100,200].index(page_size), help="èª¿æ•´æ¯é é¡¯ç¤ºçš„åˆ—æ•¸ã€‚")
        if new_size != page_size:
            st.session_state.detail_page_size = int(new_size)
            st.session_state.detail_page = 1  # æ›æ¯é æ•¸å›åˆ°ç¬¬ä¸€é 
            st.rerun()
    with c[1]:
        n1, n2, n3, n4 = st.columns([1,1,1,1])
        if n1.button("â® å›åˆ°ç¬¬ä¸€é ", disabled=(page <= 1)):
            st.session_state.detail_page = 1
            st.rerun()
        if n2.button("Â« ä¸Šä¸€é ", disabled=(page <= 1)):
            st.session_state.detail_page = max(1, page - 1)
            st.rerun()
        if n3.button("ä¸‹ä¸€é  Â»", disabled=(page >= total_pages)):
            st.session_state.detail_page = min(total_pages, page + 1)
            st.rerun()
        if n4.button("è·³åˆ°æœ€å¾Œé  â­", disabled=(page >= total_pages)):
            st.session_state.detail_page = total_pages
            st.rerun()
    with c[2]:
        st.markdown(f"**ç¬¬ {page} / {total_pages} é **ã€€é¡¯ç¤ºç¬¬ **{start+1}â€“{end}** ç­†ï¼ˆå…± **{total_rows}** ç­†ï¼‰")

# å…¨é‡ CSV åŒ¯å‡ºï¼ˆä¸å—åˆ†é å½±éŸ¿ï¼‰
st.download_button(
    label="ä¸‹è¼‰æ˜ç´° CSVï¼ˆå…¨é‡ï¼‰",
    data=df_sorted.to_csv(index=False).encode("utf-8-sig"),
    file_name=f"today_kanban_detail_{now_local:%Y%m%d_%H%M}.csv",
    mime="text/csv",
    help="åŒ¯å‡ºç•¶å‰ç¯©é¸å¾Œçš„æ‰€æœ‰åˆ—ï¼ˆä¸å—åˆ†é å½±éŸ¿ï¼‰ã€‚"
)

if sel_rows:
    sel_idx = sel_rows[0]
    # df_display èˆ‡ df_page çš„é †åºä¸€è‡´ï¼Œsel_idx å°æ‡‰ df_page çš„ç›¸å°ä½ç½®
    if 0 <= sel_idx < len(df_page):
        row = df_page.iloc[sel_idx]
        # è‹¥é¸å–äº†èˆ‡ä¸Šæ¬¡ä¸åŒçš„åˆ—ï¼Œæ¸…é™¤ä¸Šä¸€ç­† AI åˆ†æçµæœï¼Œé¿å…æ®˜ç•™
        new_key = f"{row.get('work_order_id','')}-{row.get('kanban_id','')}"
        prev_key = st.session_state.get("current_row_key")
        if new_key != prev_key:
            st.session_state.pop("last_ai_result", None)
            st.session_state.pop("last_ai_key", None)
        # åƒ…é‡å°é€¾æœŸåˆ—
        try:
            overdue_flag = int(row.get("overdue_not_started_so_far_flag", 0))
        except Exception:
            overdue_flag = 1 if str(row.get("overdue_not_started_so_far_flag", "")).strip() in ("1","True","true","Y") else 0

        if overdue_flag != 1:
            st.info("åƒ…é‡å°ã€é€¾æœŸæœªé–‹å·¥ã€çš„åˆ—æä¾›åˆ†æã€‚è«‹é¸å–ç´…è‰²é«˜äº®åˆ—ã€‚")
        else:
            with engine.connect() as conn:
                upstream = fetch_prev_process_and_upstream(conn, row)

            # å°‡ upstream çš„æ™‚é–“æ¬„ä½ï¼ˆè‹¥ç‚º UTC æˆ– naiveï¼‰è½‰ç‚ºé¡¯ç¤ºæ™‚å€ï¼ˆä¾‹å¦‚ Asia/Taipeiï¼‰
            try:
                for tcol in ("actual_start_time", "actual_end_time"):
                    ts = upstream.get(tcol)
                    if ts is None or ts == "":
                        continue
                    s = pd.to_datetime(ts, errors="coerce")
                    if pd.isna(s):
                        continue
                    # è‹¥ç‚º naiveï¼Œè¦–ç‚º UTCï¼›è‹¥å·²æœ‰ tzï¼Œç›´æ¥è½‰é¡¯ç¤ºæ™‚å€
                    if getattr(s, "tzinfo", None) is None:
                        s = s.tz_localize("UTC").tz_convert(display_tz)
                    else:
                        s = s.tz_convert(display_tz)
                    upstream[tcol] = s.strftime("%Y-%m-%d %H:%M:%S")
            except Exception:
                pass

            # é€£ç·šè³‡æ–™åº«ï¼ˆsuppï¼‰å–å¾—ç‰©æ–™å‚™æ–™é€²åº¦åˆ—è¡¨
            material_prep_list = []
            kanban_id_val = row.get("kanban_id")
            if kanban_id_val:
                db_lps_conf = get_db_leanplay_supp_config(cp, cfg)
                lps_DB_URL = build_db_url(db_lps_conf)
                lps_engine = create_engine(lps_DB_URL, pool_pre_ping=True)
                with lps_engine.connect() as supp_conn:
                    material_prep_list = get_material_production_progress_list_of_kanban(supp_conn, kanban_id_val)

            # å°‡ç‰©æ–™æ¸…å–®çš„ load_dts è¦–ç‚º UTCï¼Œè½‰ç‚ºé¡¯ç¤ºæ™‚å€ï¼ˆä¾‹å¦‚ Asia/Taipeiï¼‰ä»¥ä¾¿å‰ç«¯é¡¯ç¤º
            if material_prep_list:
                for _item in material_prep_list:
                    ts = _item.get("load_dts")
                    if ts is None or ts == "":
                        continue
                    try:
                        s = pd.to_datetime(ts, errors="coerce")
                        if pd.isna(s):
                            continue
                        # è‹¥ç‚º naiveï¼Œå…ˆè¦–ç‚º UTCï¼Œå†è½‰ display_tzï¼›è‹¥å·²æœ‰ tzï¼Œç›´æ¥è½‰ display_tz
                        if getattr(s, "tzinfo", None) is None:
                            s = s.tz_localize("UTC").tz_convert(display_tz)
                        else:
                            s = s.tz_convert(display_tz)
                        _item["load_dts"] = s.strftime("%Y-%m-%d %H:%M:%S")
                    except Exception:
                        # ä»»ä½•è½‰æ›å¤±æ•—å‰‡ç¶­æŒåŸå€¼
                        pass
            
            payload = {
                "as_of": asof_str,
                "day_start": f"{now_local.strftime('%Y-%m-%d')} 00:00:00 ({display_tz})",
                "day_end":   f"{now_local.strftime('%Y-%m-%d')} 24:00:00 ({display_tz})",
                "task": {
                    "kanban_id": row.get("kanban_id"),
                    "work_order_id": row.get("work_order_id"),
                    "part_no": row.get("part_no"),
                    "work_center_id": row.get("work_center_id"),
                    "process_id": row.get("process_id"),
                    "process_seq": int(row.get("process_seq",0)),
                    "expected_start_time": str(row.get("expected_start_time")),
                    "actual_start_time": str(row.get("actual_start_time")),
                    "produce_status": int(row.get("produce_status",0)) if pd.notna(row.get("produce_status")) else None,
                    "flags": {
                        "planned_today_flag": int(row.get("planned_today_flag",0)),
                        "overdue_not_started_so_far_flag": int(row.get("overdue_not_started_so_far_flag",0)),
                        "started_on_time_today_flag": int(row.get("started_on_time_today_flag",0)),
                        "started_late_today_flag": int(row.get("started_late_today_flag",0))
                    }
                },
                "context": {
                    "upstream_status": dict(upstream) if upstream else None,
                    "materials_prep_status": material_prep_list
                }
            }
            st.success(f"å·²çµ„æˆ AI åˆ†æ payloadï¼š{row.get('work_order_id','')}-{row.get('kanban_id','')}ï¼Œè«‹è‡³ä¸‹æ–¹ **Payload** åˆ†é æŸ¥çœ‹æˆ–ä¸‹è¼‰ã€‚")
            # å°‡æœ¬æ¬¡é¸å–çš„ payload/row key å­˜å…¥ sessionï¼Œä¾›ä¸‹æ–¹ Tabs ä½¿ç”¨
            st.session_state["current_payload"] = payload
            st.session_state["current_row_key"] = f"{row.get('work_order_id','')}-{row.get('kanban_id','')}"


# --- é€çµ¦ AIï¼ˆç¤ºä¾‹éª¨æ¶ï¼‰---

# å»ºç«‹çµæœå®¹å™¨ï¼ˆå°±åœ¨ä½ é¡¯ç¤º payload çš„é™„è¿‘ï¼‰
tabs = st.tabs(["AI åˆ†æ", "Payload"])

# å–å‡ºæš«å­˜ payload/keyï¼Œä¾›å„ Tab ä½¿ç”¨
current_payload = st.session_state.get("current_payload")
current_row_key = st.session_state.get("current_row_key", "")

with tabs[0]:
    # ä¸€éµé€å‡º
    send_col1, send_col2 = st.columns([1,4])
    with send_col1:
        run = st.button("ğŸš€ é€ AI åˆ†æ", type="primary", width='stretch', key="run_ai")
    with send_col2:
        st.caption("é»é¸è¡¨æ ¼ä¸€åˆ—å¾Œï¼Œå¯åœ¨æ­¤é€å‡ºä¸¦æŸ¥çœ‹çµè«–ã€‚")

    if run and current_payload:
        with st.spinner("AI åˆ†æä¸­â€¦"):
            result = analyze_overdue_with_llm(current_payload, stream=False)
        st.session_state["last_ai_result"] = result
        # ä»¥ session ä¸­çš„ keyï¼ˆè‹¥ç„¡ï¼Œå¾ payload.task çµ„ï¼‰
        key_from_payload = ""
        try:
            t = current_payload.get("task", {}) if isinstance(current_payload, dict) else {}
            key_from_payload = f"{t.get('work_order_id','')}-{t.get('process_id','')}-{t.get('process_seq','')}"
        except Exception:
            key_from_payload = ""
        st.session_state["last_ai_key"] = current_row_key or key_from_payload
    elif run and not current_payload:
        st.warning("è«‹å…ˆåœ¨è¡¨æ ¼ä¸­é¸å–ä¸€åˆ—ï¼ˆé€¾æœŸæœªé–‹å·¥ï¼‰ä»¥ç”¢ç”Ÿåˆ†æ Payloadã€‚")

    result = st.session_state.get("last_ai_result")
    if result:
        # å¦‚æœ LLM å‘¼å«å±¤å›å‚³äº†éŒ¯èª¤ï¼Œå…ˆé¡¯ç¤ºæé†’
        if result.get("error"):
            st.warning(f"LLM å‘¼å«å¤±æ•—ï¼š{result['error']}")

        st.success(f"åˆ†æå®Œæˆï¼ˆ{st.session_state.get('last_ai_key', current_row_key)}ï¼‰")
        st.markdown(f"### çµè«–\n{result.get('summary','')}")

        analysis = result.get("analysis", {}) if isinstance(result, dict) else {}
        root_items = analysis.get("root_causes", []) or []

        k1, k2, k3 = st.columns(3)
        with k1:
            st.markdown("**ç–‘ä¼¼åŸå› **")
            bullets = []
            for it in root_items:
                if not isinstance(it, dict):
                    bullets.append(f"* {str(it)}")
                    continue
                title = it.get("title") or "(æœªå‘½ååŸå› )"
                conf = it.get("confidence")
                if isinstance(conf, (int, float)):
                    title = f"{title}ï¼ˆä¿¡å¿ƒ {float(conf):.0%}ï¼‰"
                bullets.append(f"* {title}")
                for s in (it.get("signals") or []):
                    bullets.append(f"  * {s}")
            if not bullets:
                bullets = ["ï¼ˆæ¨¡å‹æœªæä¾›åŸå› æ‘˜è¦ï¼‰"]
            st.write("\n".join(bullets))

        with k2:
            st.markdown("**å»ºè­°è¡Œå‹•**")

            # ---- çŸ­æœŸæ”¹å–„ï¼šæ²¿ç”¨ LLM analysis çš„ follow_up_queries ----
            st.markdown("**çŸ­æœŸæ”¹å–„**")
            short_actions = analysis.get("follow_up_queries") or []
            if short_actions:
                st.write("\n".join([f"* {a}" for a in short_actions]))
            else:
                st.caption("ï¼ˆå°šç„¡çŸ­æœŸå»ºè­°ï¼‰")

            # ---- é•·æœŸæ”¹å–„ï¼šRAG å¾ç²¾å¯¦ç”Ÿç”¢ç†è«–è£œå…… ----
            st.markdown("**é•·æœŸæ”¹å–„ï¼ˆç²¾å¯¦ç”Ÿç”¢çŸ¥è­˜åº«ï¼‰**")
            long_actions: list[str] = []
            rag_err = None

            # ä»¥é¸å–åˆ—çš„å”¯ä¸€éµç•¶ä½œå¿«å– keyï¼Œé¿å…æ¯æ¬¡é‡ç®—
            rag_key = st.session_state.get("last_ai_key") or st.session_state.get("current_row_key") or ""
            cached_key = st.session_state.get("last_rag_key")
            cached_res = st.session_state.get("last_rag_result")

            if rag_key and cached_key == rag_key and cached_res is not None:
                long_actions = cached_res.get("long_term_actions") or []
                rag_err = cached_res.get("error")
            else:
                # å³æ™‚å‘¼å« RAGï¼ˆä»¥ LLM analysis ç‚ºè¼¸å…¥ï¼Œå¯é™„å¸¶ payload ä½œç‚ºèƒŒæ™¯ï¼‰
                with st.spinner("æŸ¥è©¢é•·æœŸæ”¹å–„å»ºè­°ï¼ˆRAGï¼‰â€¦"):
                    rag_res = get_long_term_improvements(analysis, current_payload)
                st.session_state["last_rag_result"] = rag_res
                st.session_state["last_rag_key"] = rag_key
                long_actions = rag_res.get("long_term_actions") or []
                rag_err = rag_res.get("error")

            if rag_err:
                st.warning(f"RAG å–å¾—å»ºè­°æ™‚ç™¼ç”Ÿå•é¡Œï¼š{rag_err}")
            if long_actions:
                st.write("\n".join([f"* {x}" for x in long_actions]))
            else:
                st.caption("ï¼ˆæš«ç„¡é•·æœŸå»ºè­°ï¼‰")

        with k3:
            st.markdown("**é—œéµä¾æ“š**")
            ev = {
                "upstream_status": current_payload.get("context", {}).get("upstream_status") if current_payload else None,
                "materials_prep_status": current_payload.get("context", {}).get("materials_prep_status") if current_payload else None,
            }
            st.json(ev, expanded=False)

        # ä¸‹è¼‰éˆ•ï¼ˆå«å®Œæ•´ resultï¼‰
        st.download_button(
            "ä¸‹è¼‰åˆ†æçµæœï¼ˆJSONï¼‰",
            data=json.dumps(_json_safe(result), ensure_ascii=False, indent=2),
            file_name=f"ai_analysis_{now_local:%Y%m%d_%H%M}.json",
            mime="application/json"
        )


with tabs[1]:
    if current_payload:
        st.code(json.dumps(_json_safe(current_payload), ensure_ascii=False, indent=2), language="json")
    else:
        st.info("å°šæœªç”¢ç”Ÿ Payloadã€‚è«‹åœ¨ä¸Šæ–¹è¡¨æ ¼é¸å–ä¸€åˆ—ï¼ˆé€¾æœŸæœªé–‹å·¥ï¼‰ä»¥å»ºç«‹åˆ†æè³‡æ–™ã€‚")

# # ----------------------------
# # 7) å´é‚Šå·¥å…·
# # ----------------------------
# with st.sidebar:
#     st.markdown("### å·¥å…·")
#     if st.button("é‡æ–°æ•´ç†"):
#         st.cache_data.clear()
#         st.rerun()